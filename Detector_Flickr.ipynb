{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "\n",
    "SEMILLA_ALEATORIEDAD = 123\n",
    "np.random.seed(SEMILLA_ALEATORIEDAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d760fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import set_option\n",
    "#from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eff49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un gráfico circular con el porcentaje de fotógrafos prefesionales y los que no\n",
    "def show_loan_distrib(y):\n",
    "    i=0\n",
    "    normales = 0\n",
    "    fotografos = 0\n",
    "    while (i<len(y)):\n",
    "        if y[i]==0:\n",
    "            normales+=1\n",
    "        else: fotografos+=1\n",
    "        i+=1\n",
    "\n",
    "    tamanos = [fotografos, normales]\n",
    "    etiquetas = [f'Fotógrafos profesionales - {tamanos[0]}', f'Otros usuarios - {tamanos[1]}']\n",
    "\n",
    "    plt.pie(tamanos, labels=etiquetas, autopct = '%1.1f%%', explode = [0, 0.1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee26cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para representar la matriz de confusión de las etiquetas\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def matrix_confusion_etiq(etiq_manual, etiq_auto):\n",
    "    cm = confusion_matrix(etiq_manual, etiq_auto) \n",
    "    print(cm)\n",
    "    \n",
    "    # Para mostrar la información en porcetanjes también\n",
    "    cm_porcentaje = cm.astype('float')/cm.sum()*100\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    for (i, j), val in np.ndenumerate(cm):\n",
    "        percentage = cm_porcentaje[i, j]\n",
    "        if i==0 and j==0:\n",
    "            ax.text(j, i, f'{val} ({percentage:.2f}%)', ha='center', va='center', color = 'white')\n",
    "        else:\n",
    "            ax.text(j, i, f'{val} ({percentage:.2f}%)', ha='center', va='center', color = 'black')\n",
    "\n",
    "\n",
    "    labels = ['No Fotógrafo', 'Fotógrafo']\n",
    "\n",
    "    ax.set_xlabel('Etiqueta Automática')\n",
    "    ax.set_ylabel('Etiqueta Manual')\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_title('Matriz de Confusión de etiquetados en Flickr')\n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1461eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para representar la matriz de confusión de las predicciones\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def matrix_confusion_pred(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred) \n",
    "    print(cm)\n",
    "    \n",
    "    # Para mostrar la información en porcetanjes también\n",
    "    #cm_porcentaje = cm.astype('float')/cm.sum()*100\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    for (i, j), val in np.ndenumerate(cm):\n",
    "        #percentage = cm_porcentaje[i, j]\n",
    "        fontsize = 12\n",
    "        color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "        ax.text(j, i, f'{val}', ha='center', va='center', color=color, fontsize=fontsize)\n",
    "\n",
    "    labels = ['No Fotógrafo', 'Fotógrafo']\n",
    "\n",
    "    ax.set_xlabel('Predicción')\n",
    "    ax.set_ylabel('Ground-truth')\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_title('Matriz de confusión de la predicción de expertos')\n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que plotea la curva ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def roc_auc(y_test, y_pred_test_proba):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_test_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    #plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random guess (AUC = 0.5)')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b195131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que plotea la curva Precision-Recall\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from numpy import argmax\n",
    "\n",
    "def pr_auc(y_test, y_pred_test_proba):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_test_proba)\n",
    "    \n",
    "    # convert to f score\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    # locate the index of the largest f score\n",
    "    ix = argmax(fscore)\n",
    "    \n",
    "    pr_auc = auc(recall, precision)\n",
    "    # locate the index of the largest f score\n",
    "    ix = argmax(fscore)\n",
    "    print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "    # plot the roc curve for the model\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "    plt.plot(recall, precision, marker='.', label= f'PR curve (area = {pr_auc:.2f})')\n",
    "    plt.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')\n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    return thresholds[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe7c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que representa en un diagrama de barras las medidas de desempeño en la predicción\n",
    "\n",
    "# valores es el array con los valores de las métricas que queremos plotear\n",
    "# ponemos el título del plot como argumento para especificar el caso que estamos ploteando\n",
    "def show_bars_metrics(valores, titulo):\n",
    "    nombres = [\"Accuracy\",\"Precision\",\"Recall\",\"f1\", \"ROC-AUC\", \"AP/PR-AUC\"]\n",
    "    color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "    # Nombres y valores de las medidas\n",
    "    #valores = fila_max_manual.values[2:7]\n",
    "\n",
    "    # Crear una figura y un eje\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Crear la gráfica de barras\n",
    "    bars = ax.bar(nombres, valores, color=color, width=0.65)\n",
    "\n",
    "    # Agregar etiquetas de valores a las barras\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width() / 2, height), \n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=16)\n",
    "\n",
    "    # Agregar título y etiquetas de ejes\n",
    "    ax.set_title(titulo, fontweight='bold', fontsize=20)\n",
    "    ax.set_xlabel('Medidas', fontsize=12)\n",
    "    ax.set_ylabel('Valores', fontsize=12)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    # Mostrar la gráfica\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que dibuja las distribuciones de las características individuales, para cada tipo de etiquetado\n",
    "\n",
    "# str_etiq es el string del nombre que tiene la variable de clase en el data set df\n",
    "def show_individual_distrib(df, str_etiq):\n",
    "    df_prof = df[df[str_etiq] == 1]\n",
    "    df_noProf = df[df[str_etiq] == 0]\n",
    "\n",
    "    # Quitamos la columna de respuesta\n",
    "    df_dist = df_manual.iloc[:,:-1]\n",
    "\n",
    "    # Calculamos el número de filas y columnas necesarias para organizar los subtrazados\n",
    "    n_rows = (len(df_dist.columns) + 3) // 4  # Aproximadamente 4 subplots por fila\n",
    "    n_cols = min(len(df_dist.columns), 4)\n",
    "\n",
    "    # Ajustamos el tamaño de la figura\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 4 * n_rows))\n",
    "\n",
    "    # Ploteamos la distribución de cada columna en un subplot separado\n",
    "    for i, column in enumerate(df_dist.columns):\n",
    "        ax = axes[i // n_cols, i % n_cols]  # Calculamos la posición del subplot en la cuadrícula\n",
    "        # Cada diagrama tendrá 40 barras como máximo\n",
    "        df_prof[column].hist(bins=40, color = 'orange', alpha = 0.5, ax=ax)\n",
    "        df_noProf[column].hist(bins=40, color = 'blue', alpha = 0.5,ax=ax)\n",
    "        ax.set_title('{}'.format(column))\n",
    "        ax.set_xlabel('Valor')\n",
    "        ax.set_ylabel('Frecuencia')\n",
    "\n",
    "    # Ajusta automáticamente el diseño de la figura para evitar superposiciones\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818f7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos devuelve la información sobre la asimetría y la curtosis\n",
    "\n",
    "# Nos devuelve el skewness y la kurtosis de cada característica\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def info_asymetry(df):\n",
    "    skewness = df.apply(skew)\n",
    "    kurt = df.apply(kurtosis)\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        'Feature': df.columns,\n",
    "        'Skewness': skewness,\n",
    "        'Kurtosis': kurt\n",
    "    }).reset_index(drop=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b594332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dibuja la matriz de correlación de Pearson entre los atributos\n",
    "\n",
    "def show_correlation(df):\n",
    "    correlation_matrix=df.corr().abs()\n",
    "    sns.set(font_scale = 0.8)\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "    sns.heatmap(correlation_matrix, cmap='YlGnBu', square=True, ax = ax)\n",
    "\n",
    "    # Rotar las etiquetas del eje x\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partición de los datos\n",
    "\n",
    "def data_partition(df, proportion, str_etiq):\n",
    "    train_df, test_df = train_test_split(df, test_size=proportion, random_state=SEMILLA_ALEATORIEDAD, stratify=df[str_etiq])\n",
    "\n",
    "    # Separamos la variable de clase de las demás\n",
    "    train_manual_x = train_df.loc[:, train_df.columns != str_etiq]\n",
    "    train_manual_y = train_df.loc[:, str_etiq]\n",
    "\n",
    "    test_manual_x = test_df.loc[:, test_df.columns != str_etiq]\n",
    "    test_manual_y = test_df.loc[:, str_etiq]\n",
    "    \n",
    "    return [train_manual_x, train_manual_y, test_manual_x, test_manual_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado con StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Escalado estándar\n",
    "def std_Scaler_data(train_x, test_x):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_x)\n",
    "    trainScaled_x = pd.DataFrame(scaler.transform(train_x), columns=train_x.columns, index=train_x.index)\n",
    "    testScaled_x = pd.DataFrame(scaler.transform(test_x), columns=test_x.columns, index=test_x.index)\n",
    "    return [trainScaled_x, testScaled_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e272812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado con QuantileTransformer\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# QuantileTransformer\n",
    "def qt_Transformer_data(train_x, test_x):\n",
    "    \n",
    "    qt = QuantileTransformer(output_distribution='normal')\n",
    "    qt.fit(train_x)\n",
    "    trainQT_x = pd.DataFrame(qt.transform(train_x), columns=train_x.columns, index=train_x.index)\n",
    "    testQT_x = pd.DataFrame(qt.transform(test_x), columns=test_x.columns, index=test_x.index)\n",
    "    return [trainQT_x, testQT_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase que representa el transformador donde se elimina del conjunto las columnas con valores constantes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ConstantValueThresholdEliminator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.9):\n",
    "        self.threshold = threshold\n",
    "        self.columns_to_drop = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calcular el umbral de valores constantes para cada columna\n",
    "        for column in X.columns:\n",
    "            most_frequent_value_count = X[column].value_counts().max()\n",
    "            if most_frequent_value_count / len(X) > self.threshold:\n",
    "                self.columns_to_drop.append(column)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Eliminar las columnas que exceden el umbral de valores constantes\n",
    "        return X.drop(columns=self.columns_to_drop)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f544e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que hace uso de la clase ConstantValueThresholdEliminator para eliminar columnas constantes\n",
    "\n",
    "def constant_elimination(train_x, test_x, threshold):\n",
    "    transformer_cte = ConstantValueThresholdEliminator(threshold=threshold)\n",
    "    transformer_cte.fit(train_x)\n",
    "    train_cte_x = transformer_cte.transform(train_x)\n",
    "    test_cte_x = transformer_cte.transform(test_x)\n",
    "    return [train_cte_x, test_cte_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae0b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase que representa el transformador donde se elimina del conjunto las columnas más correlacionadas\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CorrelationThresholdTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.8):\n",
    "        self.threshold = threshold\n",
    "        self.features_to_keep_ = None\n",
    "        self.to_remove = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Convertimos a DataFrame si no lo es\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        # Calculamos la matriz de correlación\n",
    "        corr_matrix = X.corr().abs()\n",
    "\n",
    "        # Calculamos la correlación media de cada variable con el resto\n",
    "        mean_correlation = corr_matrix.mean(axis=0)\n",
    "\n",
    "        # Identificamos las variables a eliminar basadas en el umbral\n",
    "        self.to_remove = set()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i + 1, len(corr_matrix.columns)):\n",
    "                if corr_matrix.iloc[i, j] > self.threshold:\n",
    "                    if mean_correlation.iloc[i] > mean_correlation.iloc[j]:\n",
    "                        self.to_remove.add(corr_matrix.columns[i])\n",
    "                    else:\n",
    "                        self.to_remove.add(corr_matrix.columns[j])\n",
    "\n",
    "        # Guardamos las variables a mantener\n",
    "        self.features_to_keep_ = [col for col in X.columns if col not in self.to_remove]\n",
    "        return self\n",
    "    def get_removed(self):\n",
    "        return self.to_remove\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Convertimos a DataFrame si no lo es\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        # Retornamos el dataset con las variables seleccionadas\n",
    "        return X[self.features_to_keep_]\n",
    "\n",
    "    def get_support(self):\n",
    "        # Método para obtener las características seleccionadas\n",
    "        return self.features_to_keep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que usa CorrelationThresholdTransformer para eliminar columnas correlacionadas\n",
    "\n",
    "def corr_elimination(train_x, test_x, threshold):\n",
    "    transformer_corr = CorrelationThresholdTransformer(threshold=threshold)\n",
    "    transformer_corr.fit(train_x)\n",
    "    train_corr_x = transformer_corr.transform(train_x)\n",
    "    test_corr_x = transformer_corr.transform(test_x)\n",
    "    #print(transformer_corr.get_removed())\n",
    "    return [train_corr_x, test_corr_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be143619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que usa un entrenamiento de Random Forest para seleccionar atributos poco relevantes en el entrenamiento\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def RelevantInformationEliminator(X, y, threshold=0.01):\n",
    "    clf = RandomForestClassifier(n_estimators=250,\n",
    "                              random_state=SEMILLA_ALEATORIEDAD)\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot feature importance\n",
    "    feature_importance = clf.feature_importances_\n",
    "    # make importances relative to max importance\n",
    "    #feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    \n",
    "    features_eliminated = []\n",
    "    for index in sorted_idx:\n",
    "        if feature_importance[index] < threshold:\n",
    "            features_eliminated.append(X.columns[index])\n",
    "\n",
    "    return features_eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8badf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que elimina los atributos poco relevantes usando la función RelevantInformationEliminator\n",
    "\n",
    "def low_relevance_elimination(train_x, train_y, test_x, threshold):\n",
    "    # Seleccionamos las columnas poco relevantes\n",
    "    features_eliminated = RelevantInformationEliminator(train_x, train_y, threshold)\n",
    "    print(features_eliminated)\n",
    "    train_rel_x = train_x.drop(columns = features_eliminated)\n",
    "    test_rel_x = test_x.drop(columns = features_eliminated)\n",
    "    return [train_rel_x, test_rel_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5371cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que aplica PCA al conjunto de datos\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def dataset_pca(train_x, test_x, var_explicated):\n",
    "    pca = PCA(n_components = var_explicated)\n",
    "    train_pca_x = pca.fit_transform(train_x)\n",
    "    test_pca_x = pca.transform(test_x)\n",
    "\n",
    "    train_pca_x = pd.DataFrame(train_pca_x)\n",
    "    test_pca_x = pd.DataFrame(test_pca_x)\n",
    "    \n",
    "    return [train_pca_x, test_pca_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23db0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que aplica SMOTE al conjunto de datos\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def dataset_smote(train_x, train_y, with_sampling_strategy, k_neighbors):\n",
    "    if with_sampling_strategy and k_neighbors != 0:\n",
    "        oversample = SMOTE(random_state=SEMILLA_ALEATORIEDAD, sampling_strategy=0.5, k_neighbors=k_neighbors)\n",
    "    elif with_sampling_strategy:\n",
    "        oversample = SMOTE(random_state=SEMILLA_ALEATORIEDAD, sampling_strategy=0.5)\n",
    "    else:\n",
    "        oversample = SMOTE(random_state=SEMILLA_ALEATORIEDAD)\n",
    "        \n",
    "    smote_x, smote_y = oversample.fit_resample(train_x, train_y)\n",
    "    return [smote_x, smote_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que aplica el bloque de preprocesamiento al conjunto de datos\n",
    "\n",
    "def dataset_bloque(train_x, train_y, test_x, threshold_cte, threshold_corr,\n",
    "                  threshold_relevance):\n",
    "    train_cte_x, test_cte_x = constant_elimination(train_x, test_x, threshold_cte)\n",
    "    \n",
    "    train_corr_cte_x, test_corr_cte_x = corr_elimination(train_cte_x, test_cte_x, threshold_corr)\n",
    "    \n",
    "    train_bloque_x, test_bloque_x = low_relevance_elimination(train_corr_cte_x, train_y, test_corr_cte_x, threshold_relevance)\n",
    "    \n",
    "    return [train_bloque_x, test_bloque_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb999f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que crea la lista de conjuntos de datos con preprocesamientos distintos\n",
    "\n",
    "def conjuntos_preprocesamiento(train_x, test_x, train_y, test_y, with_pca=False,\n",
    "                              with_smote=False, threshold_cte = 0.9, threshold_corr=0.9,\n",
    "                              threshold_relevance = 0.01, sampling_strategy = False, k_neighbors = 0,\n",
    "                              var_explicated = 0.9):\n",
    "    conjuntos = []\n",
    "    #conjuntos_aux = []\n",
    "    \n",
    "    trainScaled_x, testScaled_x = std_Scaler_data(train_x, test_x)\n",
    "    conjuntos.append([\"std\", trainScaled_x, train_y, testScaled_x, test_y])\n",
    "    \n",
    "    trainScaled_bloque_x, testScaled_bloque_x = dataset_bloque(trainScaled_x, train_y, testScaled_x, threshold_cte,\n",
    "                                                           threshold_corr, threshold_relevance)\n",
    "    \n",
    "    conjuntos.append([\"std_bloque\", trainScaled_bloque_x, train_y, testScaled_bloque_x, test_y])\n",
    "    \n",
    "    #conjuntos_aux.append([\"std_bloque\", trainScaled_bloque_x, train_y, testScaled_bloque_x, test_y])\n",
    "    \n",
    "    trainQT_x, testQT_x = qt_Transformer_data(train_x, test_x)\n",
    "    conjuntos.append([\"qt\", trainQT_x, train_y, testQT_x, test_y])\n",
    "    \n",
    "    trainQT_bloque_x, testQT_bloque_x = dataset_bloque(trainQT_x, train_y, testQT_x, threshold_cte,\n",
    "                                                           threshold_corr, threshold_relevance)\n",
    "    \n",
    "    conjuntos.append([\"qt_bloque\", trainQT_bloque_x, train_y, testQT_bloque_x, test_y])\n",
    "    \n",
    "    #conjuntos_aux.append([\"qt_bloque\", trainQT_bloque_x, train_y, testQT_bloque_x, test_y])\n",
    "    \n",
    "    if with_pca and with_smote:\n",
    "        train_std_pca_x, test_std_pca_x = dataset_pca(trainScaled_bloque_x, testScaled_bloque_x, var_explicated)\n",
    "        conjuntos.append([\"std_bloque_pca\", train_std_pca_x, train_y, test_std_pca_x, test_y])\n",
    "        \n",
    "        train_qt_pca_x, test_qt_pca_x = dataset_pca(trainQT_bloque_x, testQT_bloque_x, var_explicated)\n",
    "        conjuntos.append([\"qt_bloque_pca\", train_qt_pca_x, train_y, test_qt_pca_x, test_y])\n",
    "        \n",
    "        # Aplicamos SMOTE con PCA\n",
    "        smote_std_pca_x, smote_std_pca_y = dataset_smote(train_std_pca_x, train_y, sampling_strategy, k_neighbors)\n",
    "        conjuntos.append([\"std_bloque_pca_smote\", smote_std_pca_x, smote_std_pca_y, test_std_pca_x, test_y])\n",
    "        \n",
    "        smote_qt_pca_x, smote_qt_pca_y = dataset_smote(train_qt_pca_x, train_y, sampling_strategy, k_neighbors)\n",
    "        conjuntos.append([\"qt_bloque_pca_smote\", smote_qt_pca_x, smote_qt_pca_y, test_qt_pca_x, test_y])\n",
    "\n",
    "        # Aplicamos SMOTE sin PCA\n",
    "        smote_std_x, smote_std_y = dataset_smote(trainScaled_bloque_x, train_y, sampling_strategy, k_neighbors)\n",
    "        conjuntos.append([\"std_bloque_smote\", smote_std_x, smote_std_y, testScaled_bloque_x, test_y])\n",
    "        \n",
    "        smote_qt_x, smote_qt_y = dataset_smote(trainQT_bloque_x, train_y, sampling_strategy, k_neighbors)\n",
    "        conjuntos.append([\"qt_bloque_smote\", smote_qt_x, smote_qt_y, testQT_bloque_x, test_y])\n",
    "        \n",
    "    elif with_smote:\n",
    "        smote_std_x, smote_std_y = dataset_smote(trainScaled_bloque_x, train_y, sampling_strategy, k_neighbors)\n",
    "        conjuntos.append([\"std_bloque_smote\", smote_std_x, smote_std_y, testScaled_bloque_x, test_y])\n",
    "        \n",
    "        smote_qt_x, smote_qt_y = dataset_smote(trainQT_bloque_x, train_y, sampling_strategy, k_neighbors)\n",
    "        conjuntos.append([\"qt_bloque_smote\", smote_qt_x, smote_qt_y, testQT_bloque_x, test_y])\n",
    "    elif with_pca:\n",
    "        train_std_pca_x, test_std_pca_x = dataset_pca(trainScaled_bloque_x, testScaled_bloque_x, var_explicated)\n",
    "        conjuntos.append([\"std_bloque_pca\", train_std_pca_x, train_y, test_std_pca_x, test_y])\n",
    "        \n",
    "        train_qt_pca_x, test_qt_pca_x = dataset_pca(trainQT_bloque_x, testQT_bloque_x, var_explicated)\n",
    "        conjuntos.append([\"qt_bloque_pca\", train_qt_pca_x, train_y, test_qt_pca_x, test_y])\n",
    "    \n",
    "        \n",
    "    return conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que establece los algoritmos de aprendizaje supervisado a usar\n",
    "\n",
    "# Spot-Check Algorithms\n",
    "def ImbGetBasedModelDef():\n",
    "    basedModels = []\n",
    "    # Lineales (estoy pensando en usar LR en lugar de esta)\n",
    "    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n",
    "    # No lineales\n",
    "    basedModels.append(('SVM'  , SVC(probability=True, class_weight='balanced')))\n",
    "    # Ensamblados\n",
    "    basedModels.append(('RF'   , RandomForestClassifier(class_weight='balanced')))\n",
    "    basedModels.append(('GBM'  , GradientBoostingClassifier()))\n",
    "    \n",
    "    return basedModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a244b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que aplica el bucle general de selección de modelos a partir de una búsqueda de hiperparámetros, para cada combinación\n",
    "# de algoritmo y preprocesado\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# El scoring por defecto es roc-auc, pero en clases desbalanceadas deberemos poner \"average_precision\"\n",
    "def bucle_proceso_general(conjuntos,  grids, scoring = \"roc_auc\"):\n",
    "    resultados = pd.DataFrame(columns = [\"modelo\", \"conjunto\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"AUC-ROC\",\"AP/PR-AUC\"])\n",
    "    \n",
    "    # Todo lo que nos aporta información sobre la predicción\n",
    "    best_models = []\n",
    "    best_hyperparams = []\n",
    "    matrices_confusion = []\n",
    "    y_pred_list = []\n",
    "    y_pred_proba_list = []\n",
    "    \n",
    "    imbModels = ImbGetBasedModelDef()\n",
    "    \n",
    "    i = 0\n",
    "    for model_name, model in imbModels:\n",
    "        for cjto_name, x_train, y_train, x_test, y_test in conjuntos:\n",
    "\n",
    "            grid_search = GridSearchCV(model, scoring=scoring, n_jobs= -1, verbose = 1, param_grid=grids[i], cv=5)\n",
    "            grid_search.fit(x_train, y_train)\n",
    "            \n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_models.append([model_name+\"-\"+cjto_name, best_model])\n",
    "            \n",
    "            best_params = grid_search.best_params_\n",
    "            best_hyperparams.append([model_name+\"-\"+cjto_name, best_params])\n",
    "\n",
    "            y_pred = best_model.predict(x_test)\n",
    "            y_pred_list.append([model_name+\"-\"+cjto_name, y_pred])\n",
    "            \n",
    "            y_pred_proba = best_model.predict_proba(x_test)[:,-1]\n",
    "            y_pred_proba_list.append([model_name+\"-\"+cjto_name, y_pred_proba])\n",
    "\n",
    "            print(model_name + \" \" + cjto_name)\n",
    "            print()\n",
    "            \n",
    "            cm =metrics.confusion_matrix(y_test, y_pred)\n",
    "            print(cm)\n",
    "            \n",
    "            matrices_confusion.append([model_name+\"-\"+cjto_name, cm])\n",
    "            print(metrics.classification_report(y_test,y_pred))\n",
    "\n",
    "            roc_auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "            print(\"ROC-AUC\", round(roc_auc, 4))\n",
    "            \n",
    "            average_precision = metrics.average_precision_score(y_test, y_pred_proba)\n",
    "            print(\"AP/PR-AUC: \",round(average_precision,4))\n",
    "\n",
    "            # Calcular las métricas de rendimiento\n",
    "            accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "            precision = metrics.precision_score(y_test, y_pred)\n",
    "            recall = metrics.recall_score(y_test, y_pred)\n",
    "            f1 = metrics.f1_score(y_test, y_pred)\n",
    "            resultados.loc[len(resultados.index)] = [model_name, cjto_name, accuracy, precision, recall, f1, roc_auc, average_precision]\n",
    "\n",
    "        i+=1\n",
    "        \n",
    "    return [resultados, best_models, best_hyperparams, matrices_confusion, y_pred_list, y_pred_proba_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar que nos ayuda a encontrar el mejor modelo de la lista de modelos devuelta por el proceso de la función bucle_proceso_general\n",
    "\n",
    "# nombre_buscado es el nombre de la mejor combinacion de preprocesado y algoritmo que buscamos en la\n",
    "# lista de mejores modelos para cada combinación\n",
    "def busca_tupla_nombre(nombre_buscado, best_models):\n",
    "    tupla = None\n",
    "    for nombre, params in best_hyperparams:\n",
    "        if nombre == nombre_buscado:\n",
    "            tupla = (nombre, params)\n",
    "            break\n",
    "    return tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8da758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que devuelve el mejor resultado de la lista de resultados de la función bucle_proceso_general\n",
    "\n",
    "# resultados es el dataframe con todos los resultados de cada combinación de preprocesado y algoritmo\n",
    "# scoring es la columna que comparamos para sacar el mejor\n",
    "def mejor_resultado(resultados, scoring):\n",
    "    idx_max = resultados[scoring].idxmax()\n",
    "    fila_max = resultados.loc[idx_max]\n",
    "    print(fila_max)\n",
    "    nombre_buscado = fila_max['modelo']+\"-\"+fila_max['conjunto']\n",
    "    print(nombre_buscado)\n",
    "    return [fila_max, nombre_buscado]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que guarda de la lista de elementos obtenidos por la función bucle_proceso_general, aquel que nos haya dado los mejores resultados\n",
    "\n",
    "def save_best(lista, file, nombre_buscado):\n",
    "    tupla = None\n",
    "    for nombre, elemento in lista:\n",
    "        if nombre == nombre_buscado:\n",
    "            tupla = (nombre, elemento)\n",
    "            break\n",
    "\n",
    "    joblib.dump(tupla, file)\n",
    "    return tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba4e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element(nombre, lista):\n",
    "    for nombre_element, element in lista:\n",
    "        if nombre_element == nombre:\n",
    "            element_return = element\n",
    "            break\n",
    "    return element_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a128d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_preproc(nombre, conjunto):\n",
    "    for nombre_aux, train_x, train_y, test_x, test_y in conjunto:\n",
    "        if nombre == nombre_aux:\n",
    "            tupla = [nombre_aux, train_x, train_y, test_x, test_y]\n",
    "            break\n",
    "    return tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que devuelve las medidas de desempeño a partir de un modelo Dummy, que depende del parámetros strategy\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "def DummyResults(train_x, train_y, test_x, test_y, strategy=\"most_frequent\"):\n",
    "    # Crear el modelo dummy\n",
    "    dummy_clf = DummyClassifier(strategy=strategy)\n",
    "\n",
    "    dummy_clf.fit(train_x, train_y)\n",
    "\n",
    "    # Hacer predicciones con el modelo dummy\n",
    "    y_pred_dummy = dummy_clf.predict(test_x)\n",
    "    y_pred_proba_dummy = dummy_clf.predict_proba(test_x)[:,-1]\n",
    "    cm =metrics.confusion_matrix(test_y, y_pred_dummy)\n",
    "    print(cm)\n",
    "\n",
    "    print(metrics.classification_report(test_y,y_pred_dummy))\n",
    "    \n",
    "    # Calcular métricas de rendimiento\n",
    "    f1 = metrics.f1_score(test_y, y_pred_dummy)\n",
    "    accuracy = metrics.accuracy_score(test_y, y_pred_dummy)\n",
    "    precision = metrics.precision_score(test_y, y_pred_dummy)\n",
    "    recall = metrics.recall_score(test_y, y_pred_dummy)\n",
    "\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    roc_auc = metrics.roc_auc_score(test_y, y_pred_proba_dummy)\n",
    "    print(\"ROC-AUC\", round(roc_auc, 4))\n",
    "    \n",
    "    average_precision = metrics.average_precision_score(test_y, y_pred_proba_dummy)\n",
    "    print(\"AP/PR-AUC: \",round(average_precision,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28bbd1",
   "metadata": {},
   "source": [
    "Para obtener el dataset con los datos para la replicación de los experimentos, consultar el README del repositorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a43eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_MANUAL_FILE = \n",
    "DATASET_AUTO_FILE ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ebddd",
   "metadata": {},
   "source": [
    "Grid de hiperparámetros a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0aac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [\n",
    "    {# LDA\n",
    "        'solver' : ['svd', 'lsqr'],\n",
    "        'n_components' : [None] + [1, 2, 5, 8, 13, 21, 34, 55],\n",
    "        'store_covariance' : [True, False],\n",
    "        'tol' : [1e-2, 1e-3, 1e-5, 1e-7, 1e-9, 1e-11]\n",
    "    },\n",
    "    {# SVM\n",
    "        'C': [1,10,100,1000],\n",
    "        'gamma': [1,0.1,0.001,0.0001], \n",
    "        'kernel': ['linear','rbf']\n",
    "    },\n",
    "    {# RF\n",
    "        'bootstrap': [True, False],\n",
    "         'max_depth': [10, 50, 100, None],\n",
    "         'max_features': ['auto', 'sqrt'],\n",
    "        'criterion' :['gini', 'entropy'],\n",
    "         'n_estimators': [200, 400, 600, 800]\n",
    "    },\n",
    "    {# GBM\n",
    "        'n_estimators': [100, 200, 500],  # Número de árboles\n",
    "        'learning_rate': [0.01, 0.1],  # Tasa de aprendizaje\n",
    "        'max_depth': [3, 5, 7],  # Profundidad máxima de los árboles\n",
    "        'max_features': ['auto', 'sqrt', 'log2']  # Fracción de características consideradas para dividir en cada nodo\n",
    "\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4fbe8",
   "metadata": {},
   "source": [
    "# Análisis previo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a42c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_csv(DATASET_MANUAL_FILE)\n",
    "df_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aut = pd.read_csv(DATASET_AUTO_FILE)\n",
    "df_aut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos el número de usuarios DELETED\n",
    "num_deleted = df_manual['is_photographer'].value_counts().get('DELETED', 0)\n",
    "print(num_deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49733acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos el número de usuarios DELETED\n",
    "num_deleted = df_aut['is_photographer'].value_counts().get('DELETED', 0)\n",
    "print(num_deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el número de valores faltantes por columna\n",
    "na_por_columna = df_manual.isna().sum()\n",
    "\n",
    "# Filtramos las columnas con valores faltantes\n",
    "columnas_con_na = na_por_columna[na_por_columna > 0]\n",
    "\n",
    "# Mostramos las columnas con valores faltantes y el número de NA\n",
    "for columna, na_count in columnas_con_na.items():\n",
    "    print(\"Columna: \"+str(columna)+ \" Número de NA: \"+str(na_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de785414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el número de valores faltantes por columna\n",
    "na_por_columna = df_aut.isna().sum()\n",
    "\n",
    "# Filtramos las columnas con valores faltantes\n",
    "columnas_con_na = na_por_columna[na_por_columna > 0]\n",
    "\n",
    "# Mostramos las columnas con valores faltantes y el número de NA\n",
    "for columna, na_count in columnas_con_na.items():\n",
    "    print(\"Columna: \"+str(columna)+ \" Número de NA: \"+str(na_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loan_distrib(df_manual['is_photographer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loan_distrib(df_aut['is_photographer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "ck_score = cohen_kappa_score(df_manual['is_photographer'], df_aut['is_photographer'])\n",
    "print(ck_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_confusion_etiq(df_manual['is_photographer'], df_aut['is_photographer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_individual_distrib(df_manual, 'is_photographer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_individual_distrib(df_aut, 'is_photographer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50885c79",
   "metadata": {},
   "source": [
    "Vamos a analizar ahora, aunque ya lo hayamos hecho de manera visual antes, la asimetría de las distribuciones, con el kurtosis y el skewness:\n",
    "\n",
    "**Skewness (Asimetría)**\n",
    "La skewness (o asimetría) mide la asimetría de la distribución de los datos alrededor de su media. Un valor de skewness proporciona información sobre la dirección y la magnitud de la asimetría:\n",
    "* **Skewness = 0**: Indica una distribución perfectamente simétrica.\n",
    "* **Skewness > 0**: Indica una distribución sesgada hacia la derecha (colas más largas a la derecha). Esto se llama skewness positiva.\n",
    "* **Skewness < 0**: Indica una distribución sesgada hacia la izquierda (colas más largas a la izquierda). Esto se llama skewness negativa.\n",
    "\n",
    "En general, valores de skewness absolutos mayores que 1 indican una fuerte asimetría, mientras que valores cercanos a 0 sugieren una distribución simétrica.\n",
    "\n",
    "**Kurtosis**\n",
    "La kurtosis mide la \"pesadez\" de las colas de la distribución en comparación con una distribución normal. Ayuda a entender si los datos tienen más o menos extremos que una distribución normal:\n",
    "* **Kurtosis = 3**: Esto indica una distribución normal, conocida como mesocúrtica.\n",
    "* **Kurtosis > 3**: Indica una distribución con colas más pesadas y picos más altos que la normal. Esta es una distribución leptocúrtica.\n",
    "* **Kurtosis < 3**: Indica una distribución con colas más ligeras y picos más bajos que la normal. Esta es una distribución platicúrtica.\n",
    "\n",
    "Valores altos de kurtosis indican un mayor riesgo de valores extremos (outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1551fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = df_manual.loc[:, df_manual.columns != \"is_photographer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ae15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "asymetry = info_asymetry(data_features)\n",
    "pd.set_option('display.max_rows', None)\n",
    "asymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f9d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_correlation(data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674bfa5a",
   "metadata": {},
   "source": [
    "# Etiqueta manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a460616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_csv(DATASET_MANUAL_FILE)\n",
    "df_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manual_x, train_manual_y, test_manual_x, test_manual_y = data_partition(df_manual, 0.3, 'is_photographer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bee572",
   "metadata": {},
   "outputs": [],
   "source": [
    "conjuntos_manual = conjuntos_preprocesamiento(train_manual_x, test_manual_x, train_manual_y, test_manual_y, with_pca=True,\n",
    "                              with_smote=True, threshold_relevance = 0.005)\n",
    "len(conjuntos_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombre, train_aux_x, train_aux_y, test_aux_x, test_aux_y in conjuntos_manual:\n",
    "    print(nombre)\n",
    "    print(str(len(train_aux_x.columns)) + \", \" + str(len(test_aux_x.columns)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e06680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if (not os.path.exists(f\"resultados_manual.csv\")):\n",
    "    resultados_manual, best_models_manual, best_hyperparams_manual, matrices_confusion_manual, y_pred_list_manual, y_pred_proba_list_manual = bucle_proceso_general(conjuntos_manual,  grids, scoring = \"average_precision\")\n",
    "    resultados_manual.to_csv(\"resultados_manual.csv\", index = False)\n",
    "    \n",
    "    fila_manual, nombre_manual = mejor_resultado(resultados_manual, \"AP/PR-AUC\")\n",
    "    \n",
    "    best_model_manual = save_best(best_models_manual, 'mejor_modelo_manual.joblib', nombre_manual)\n",
    "    best_params_manual = save_best(best_hyperparams_manual, 'mejor_params_manual.joblib', nombre_manual)\n",
    "    best_matrix_manual = save_best(matrices_confusion_manual, 'mejor_matrix_manual.joblib', nombre_manual)\n",
    "    best_ypred_manual = save_best(y_pred_list_manual, 'mejor_ypred_manual.joblib', nombre_manual)\n",
    "    best_ypred_proba_manual = save_best(y_pred_proba_list_manual, 'mejor_ypred_proba_manual.joblib', nombre_manual)\n",
    "else:\n",
    "    resultados_manual = pd.read_csv(\"resultados_manual.csv\")\n",
    "    \n",
    "    fila_manual, nombre_manual = mejor_resultado(resultados_manual, \"AP/PR-AUC\")\n",
    "    \n",
    "    best_model_manual = joblib.load('mejor_modelo_manual.joblib')\n",
    "    best_params_manual = joblib.load('mejor_params_manual.joblib')\n",
    "    best_matrix_manual = joblib.load('mejor_matrix_manual.joblib')\n",
    "    best_ypred_manual = joblib.load('mejor_ypred_manual.joblib')\n",
    "    best_ypred_proba_manual = joblib.load('mejor_ypred_proba_manual.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cee5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fila_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "DummyResults(train_manual_x, train_manual_y, test_manual_x, test_manual_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce67d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DummyResults(train_manual_x, train_manual_y, test_manual_x, test_manual_y, \"stratified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bars_metrics(fila_manual.values[2:8], \"Medidas de Desempeño para Caso Manual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_confusion_pred(test_manual_y, best_ypred_manual[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4afad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_manual = best_ypred_proba_manual[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc(test_manual_y, y_pred_proba_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_manual = pr_auc(test_manual_y, y_pred_proba_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir probabilidades a etiquetas binarias usando el umbral\n",
    "y_pred_threshold = (y_pred_proba_manual >= threshold_manual).astype(int)\n",
    "\n",
    "# Calcular métricas de rendimiento\n",
    "f1 = metrics.f1_score(test_manual_y, y_pred_threshold)\n",
    "accuracy = metrics.accuracy_score(test_manual_y, y_pred_threshold)\n",
    "precision = metrics.precision_score(test_manual_y, y_pred_threshold)\n",
    "recall = metrics.recall_score(test_manual_y, y_pred_threshold)\n",
    "\n",
    "cm =metrics.confusion_matrix(test_manual_y, y_pred_threshold)\n",
    "print(cm)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497db888",
   "metadata": {},
   "source": [
    "# Etiqueta Computacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aut = pd.read_csv(DATASET_AUTO_FILE)\n",
    "df_aut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a5f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auto_x, train_auto_y, test_auto_x, test_auto_y = data_partition(df_aut, 0.3, 'is_photographer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conjuntos_auto = conjuntos_preprocesamiento(train_auto_x, test_auto_x, train_auto_y, test_auto_y, with_pca=True,\n",
    "                              with_smote=True, threshold_relevance = 0.005, sampling_strategy = True, k_neighbors = 4)\n",
    "len(conjuntos_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombre, train_aux_x, train_aux_y, test_aux_x, test_aux_y in conjuntos_auto:\n",
    "    print(nombre)\n",
    "    print(str(len(train_aux_x.columns)) + \", \" + str(len(test_aux_x.columns)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if (not os.path.exists(f\"resultados_auto.csv\")):\n",
    "    resultados_auto, best_models_auto, best_hyperparams_auto, matrices_confusion_auto, y_pred_list_auto, y_pred_proba_list_auto = bucle_proceso_general(conjuntos_auto,  grids, scoring = \"average_precision\")\n",
    "    resultados_auto.to_csv(\"resultados_auto.csv\", index = False)\n",
    "    \n",
    "    fila_auto, nombre_auto = mejor_resultado(resultados_auto, \"AP/PR-AUC\")\n",
    "    \n",
    "    best_model_auto = save_best(best_models_auto, 'mejor_modelo_auto.joblib', nombre_auto)\n",
    "    best_params_auto = save_best(best_hyperparams_auto, 'mejor_params_auto.joblib', nombre_auto)\n",
    "    best_matrix_auto = save_best(matrices_confusion_auto, 'mejor_matrix_auto.joblib', nombre_auto)\n",
    "    best_ypred_auto = save_best(y_pred_list_auto, 'mejor_ypred_auto.joblib', nombre_auto)\n",
    "    best_ypred_proba_auto = save_best(y_pred_proba_list_auto, 'mejor_ypred_proba_auto.joblib', nombre_auto)\n",
    "else:\n",
    "    resultados_auto = pd.read_csv(\"resultados_auto.csv\")\n",
    "    \n",
    "    fila_auto, nombre_auto = mejor_resultado(resultados_auto, \"AP/PR-AUC\")\n",
    "    \n",
    "    best_model_auto = joblib.load('mejor_modelo_auto.joblib')\n",
    "    best_params_auto = joblib.load('mejor_params_auto.joblib')\n",
    "    best_matrix_auto = joblib.load('mejor_matrix_auto.joblib')\n",
    "    best_ypred_auto = joblib.load('mejor_ypred_auto.joblib')\n",
    "    best_ypred_proba_auto = joblib.load('mejor_ypred_proba_auto.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fila_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156aa067",
   "metadata": {},
   "outputs": [],
   "source": [
    "DummyResults(train_auto_x, train_auto_y, test_auto_x, test_auto_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8af4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DummyResults(train_auto_x, train_auto_y, test_auto_x, test_auto_y, \"stratified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ad2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bars_metrics(fila_auto.values[2:8], \"Medidas de Desempeño para Caso Computacional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_confusion_pred(test_auto_y, best_ypred_auto[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afddb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_auto = best_ypred_proba_auto[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bf872",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc(test_auto_y, y_pred_proba_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_auto = pr_auc(test_auto_y, y_pred_proba_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir probabilidades a etiquetas binarias usando el umbral\n",
    "y_pred_threshold = (y_pred_proba_auto >= threshold_auto).astype(int)\n",
    "\n",
    "# Calcular métricas de rendimiento\n",
    "f1 = metrics.f1_score(test_auto_y, y_pred_threshold)\n",
    "accuracy = metrics.accuracy_score(test_auto_y, y_pred_threshold)\n",
    "precision = metrics.precision_score(test_auto_y, y_pred_threshold)\n",
    "recall = metrics.recall_score(test_auto_y, y_pred_threshold)\n",
    "\n",
    "cm =metrics.confusion_matrix(test_auto_y, y_pred_threshold)\n",
    "print(cm)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb271277",
   "metadata": {},
   "source": [
    "# Estudio t de Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Realizar el t-test\n",
    "t_stat, p_value = ttest_rel(best_ypred_manual[1], best_ypred_auto[1])\n",
    "\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5eace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
